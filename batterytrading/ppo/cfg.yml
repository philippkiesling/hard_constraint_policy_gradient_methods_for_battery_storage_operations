# Config file for PPO
model:
  batch_size: 64
  clip_range: 0.2 # 0.25
  clip_range_vf: 0.2
  proj_coef: 0.0 #10.0 #3.0 # set this to 0.0 to disable the projection loss
  clip_range_proj: 0.2 # 0.1
  ent_coef: 0.01 # 0.05-0.01 0.005
  env: "continuous" #"continuous" # discrete" # "continuous" # "discrete"
  gae_lambda: 0.95
  gamma: 0.99 # 0.99
  learning_rate: 'cosine' # schedule of the learning rate (can be cosine, linear, constant)
  lr_base: 1e-4
  max_grad_norm: 0.5
  n_epochs: 10
  n_steps: 672 # 96 # 24 (hour/day) * 4 (intervals/hour) * 7 (days/week) * 4 (weeks)
  #policy: 'MlpLstmPolicyMasked'
  #policy: 'MlpLstmPolicy'
  #policy: 'MlpPolicy'
  #policy: 'MlpPolicyMasked'
  #policy: 'linearprojected'
  policy: 'clampedlstm'
  #policy: 'clampedmlp'
  #policy: 'linearprojectedlstm'
  #policy: ActivationFunctionProjectedMlpLstmPolicy
  normalize_advantage: True
  sde_sample_freq: -1 # Should be switched for basic functionalities
  use_sde: False  # Should be switched for basic functionalities
  tensorboard_log: .log/log_tb/
  verbose: 1
  target_kl:  #0.8 # Higher Value? because Early Stopping kicks in?
  device: 'cpu' #mps does not work
  create_eval_env: False
  policy_kwargs:
    log_std_init: -0.1 # Change this to higher value?
    ortho_init: True
    activation_fn: gelu
    normalize_images: False
    lstm_kwargs:
      enable_critic_lstm: True
      lstm_hidden_size: 32
      n_lstm_layers: 2  # Not the Number of timesteps to keep in the memory
      lstm_kwargs:
        #use_attention: True
        #bidirectional: True
  pretrain: False
train:
  total_timesteps: 12500000 # We have: 250000, when we want to train with 5 envs we need 1250000 = 250000 * 5
  eval_freq: 10
  log_interval: 1

env:
  max_charge: 0.15
  total_storage_capacity: 1
  initial_charge: 0.0
  max_SOC: 1
  price_time_horizon: 1.5
  data_root_path: "../"
  time_interval: "15min"
  n_past_timesteps: 1 # 24/4 timesteps corresponds to 6 hours of historic data as input
  time_features: True
  prediction_output: "action"
  day_ahead_environment: False
  max_steps: 250000
  skip_steps_interval: 672 #  2880 # Corresponds to one week
  preprocessing:
    clipaction: False
    normalizeobservation: True
    normalizereward: True
    transformobservation: True
    transformreward: True
  reward_shaping:
    type: "linear"
    base_soc_reward: 0.0 # 0.6
    base_action_taking_reward: -1 # 2
    base_consecutive_action_reward: 0.0
    base_income_reward: 1.0
    cumulative_coef: 0.0
  n_envs: 32
  gaussian_noise: False
  noise_std: None
wandb:
  use_wandb: True
  project: "batterytrading_tuneprojections"
  name: "Clamped_RPPO_New_RolloutBuffer"
  resume: "allow"
  sync_tensorboard: True
  model_save_path: .log/wandb/modelNormalizeObservationPartiallys