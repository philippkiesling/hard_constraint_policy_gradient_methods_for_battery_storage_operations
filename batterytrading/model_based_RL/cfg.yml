# Config file for PPO


model:
  batch_size: 672
  clip_range: 0.25 # 0.25
  clip_range_vf: 0.001
  ent_coef: 0.0
  #("env", "Pendulum-v1"),
  env: BatteryStorageEnv
  gae_lambda: 0.95
  gamma: 0.99
  learning_rate: schedule
  max_grad_norm: 0.5
  n_epochs: 10
  n_steps: 672 # 24 (hour/day) * 4 (intervalls/hour) * 7 (days/week) * 4 (weeks)
  #policy: 'MlpLstmPolicy'
  policy: 'MlpPolicy'
  normalize_advantage: True
  sde_sample_freq: 4
  use_sde: True
  vf_coef: 0.5
  tensorboard_log: .log/log_tb/
  verbose: 1
  policy_kwargs:
    log_std_init: -0.5 # -1.5
    ortho_init: True
    activation_fn: tanh
    lstm_kwargs:
      enable_critic_lstm: False
      lstm_hidden_size: 128

train:
  total_timesteps: 100000
  eval_freq: 100
  log_interval: 1

env:
  max_charge: 0.15
  total_storage_capacity: 1
  initial_charge: 0.0
  max_SOC: 1
  price_time_horizon: 1.5
  data_root_path: ""
  time_interval: "15min"
  n_past_timesteps: 4 # 24/4 timesteps corresponds to 6 hours of historic data as input
  time_features: True
  preprocessing:
    clipaction: True
    normalizeobservation: True
    normalizereward: True
    transformobservation: True
    transformreward: True

wandb:
  use_wandb: False
  project: "batterytrading"
  #entity: "batterytrading"
  name: "MLP"
  resume: "allow"
  sync_tensorboard: True
  model_save_path: .log/wandb/modelNormalizeObservationPartiallys
